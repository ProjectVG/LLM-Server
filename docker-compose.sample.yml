services:
  llm-server:
    build: .
    container_name: llm-server
    ports:
      - "8080:8080"
    environment:
      - SERVER_HOST=0.0.0.0
      - SERVER_PORT=8080
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-4o-mini}
      - LOG_LEVEL=${LOG_LEVEL:-INFO}
      - LOG_FILE=logs/app.log
    volumes:
      - ./logs:/app/logs
    restart: unless-stopped